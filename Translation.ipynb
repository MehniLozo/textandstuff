{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "# !curl -L -s --compressed https://www.statmt.org/europarl/v7/fr-en.tgz\n",
        "!curl -L -s --compressed https://www.statmt.org/europarl/v7/fr-en.tgz | tar -xzvf -\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGg3dTSf5swK",
        "outputId": "3b987ad1-27b8-4ce4-be03-99f3d4d9a83e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 8.11 µs\n",
            "europarl-v7.fr-en.en\n",
            "europarl-v7.fr-en.fr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "files = os.listdir('/content')\n",
        "files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWt1pkEG6tmM",
        "outputId": "4cccd45a-d291-422e-9b4c-d52980cf3412"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'europarl-v7.fr-en.en', 'europarl-v7.fr-en.fr', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EvW_B6zyhsxN"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from pickle import dump"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load(file_name):\n",
        "  file = open(file_name, mode='rt',encoding='utf-8')\n",
        "  t = file.read()\n",
        "  file.close()\n",
        "  return t"
      ],
      "metadata": {
        "id": "mSBlYy2ElwH0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentencize(doc):\n",
        "  return doc.strip().split('\\n')"
      ],
      "metadata": {
        "id": "wm8cecDgl_jR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shortest_longest_length(sentences):\n",
        "  lengths = [len(s.split()) for s in sentences]\n",
        "  return min(lengths),max(lengths)"
      ],
      "metadata": {
        "id": "lDPKZ30MmH2w"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "\n",
        "def clean_lines(lines):\n",
        "  cleaned_list= list()\n",
        "  re_print = re.compile('[^%s]'% re.escape(string.printable))\n",
        "\n",
        "  table = str.maketrans('','',string.punctuation)\n",
        "\n",
        "  for line in lines:\n",
        "    line = unicodedata.normalize('NFD',line).encode('ascii','ignore')\n",
        "    line = line.decode('UTF-8').split()\n",
        "    line = [word.lower() for word in line]\n",
        "    line = [word.translate(table) for word in line]\n",
        "    line= [re_print.sub('',w) for w in line]\n",
        "\n",
        "    line = [word for word in line if word.isalpha()]\n",
        "    cleaned_list.append(' '.join(line))\n",
        "  return cleaned_list"
      ],
      "metadata": {
        "id": "y0m7hGYumaTV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(lang,file_name):\n",
        "  doc = load(file_name)\n",
        "  sents = sentencize(doc)\n",
        "  minlen, maxlen = shortest_longest_length(sents)\n",
        "  print(lang,\": sentences = %d, min=%d, max=%d\"% (len(sents),minlen,maxlen))\n",
        "  clean = clean_lines(sents)\n",
        "  saving_file = lang+'.pkl'\n",
        "  out_file = open(saving_file,'wb')\n",
        "  pickle.dump(clean,out_file)\n",
        "  out_file.close()\n",
        "  print(saving_file, \" saved\")"
      ],
      "metadata": {
        "id": "mPSz5KEu-8qA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process('English','europarl-v7.fr-en.en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrkE94j53m9u",
        "outputId": "4945d946-ef1d-4cdb-c63d-b5e9d1330649"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English : sentences = 2007723, min=0, max=668\n",
            "English.pkl  saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process('French','europarl-v7.fr-en.fr')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHeO2Nv09li1",
        "outputId": "251cc69e-c79e-4990-80d7-ec0cdd772a50"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French : sentences = 2007723, min=0, max=693\n",
            "French.pkl  saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from collections import Counter\n",
        "\n",
        "def load_clean_sentences(file_name):\n",
        "  return load(open(file_name,'rb'))\n",
        "\n",
        "def save_clean_sentences(sents,file_name):\n",
        "  dump(sents, open(file_name,'wb'))\n",
        "  print('Saved: %s' % (file_name))\n",
        "\n",
        "def vocab_table(lines):\n",
        "  vocab = Counter()\n",
        "  for line in lines:\n",
        "    tokens = line.split()\n",
        "    vocab.update(tokens)\n",
        "  return vocab\n",
        "\n",
        "def trim_vocab(vocab, min_occurence): #Threshold\n",
        "  tokens =[ k for k, c in vocab.items() if c>= min_occurence]\n",
        "  return set(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "rTwg5Y5G-ssp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}