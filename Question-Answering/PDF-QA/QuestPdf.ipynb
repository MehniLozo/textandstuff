{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question your Pdf file for a critical answer"
      ],
      "metadata": {
        "id": "Fff6eBSDIZnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCO-pypeJPDm",
        "outputId": "6b35a589-0976-4aed-ce32-86ba87a69e86"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.10.2-py3-none-any.whl (47 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/5.6 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.21.0-py3-none-manylinux_2_17_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.3.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20221105 pdfplumber-0.10.2 pypdfium2-4.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "doc = pdfplumber.open(\"Distilled_BERT.pdf\")\n",
        "p = doc.pages[0]\n",
        "txt = p.extract_text()\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eWUe2cdJtG7",
        "outputId": "7d2d062f-0925-470a-a508-b719e82a302a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT, a distilled version of BERT: smaller,\n",
            "faster, cheaper and lighter\n",
            "VictorSANH,LysandreDEBUT,JulienCHAUMOND,ThomasWOLF\n",
            "HuggingFace\n",
            "{victor,lysandre,julien,thomas}@huggingface.co\n",
            "0202\n",
            "Abstract\n",
            "raM\n",
            "AsTransferLearningfromlarge-scalepre-trainedmodelsbecomesmoreprevalent\n",
            "inNaturalLanguageProcessing(NLP),operatingtheselargemodelsinon-the-\n",
            "edgeand/orunderconstrainedcomputationaltrainingorinferencebudgetsremains\n",
            "challenging. In this work, we propose a method to pre-train a smaller general- 1\n",
            "purposelanguagerepresentationmodel,calledDistilBERT,whichcanthenbefine-\n",
            "]LC.sc[\n",
            "tunedwithgoodperformancesonawiderangeoftaskslikeitslargercounterparts.\n",
            "Whilemostpriorworkinvestigatedtheuseofdistillationforbuildingtask-specific\n",
            "models,weleverageknowledgedistillationduringthepre-trainingphaseandshow\n",
            "thatitispossibletoreducethesizeofaBERTmodelby40%,whileretaining97%\n",
            "ofitslanguageunderstandingcapabilitiesandbeing60%faster. Toleveragethe\n",
            "inductivebiaseslearnedbylargermodelsduringpre-training,weintroduceatriple\n",
            "losscombininglanguagemodeling,distillationandcosine-distancelosses. Our 4v80110.0191:viXra\n",
            "smaller, fasterandlightermodelischeapertopre-trainandwedemonstrateits\n",
            "capabilitiesforon-devicecomputationsinaproof-of-conceptexperimentanda\n",
            "comparativeon-devicestudy.\n",
            "1 Introduction\n",
            "Thelasttwoyearshaveseentherise\n",
            "ofTransferLearningapproachesin\n",
            "NaturalLanguageProcessing(NLP)\n",
            "withlarge-scalepre-trainedlanguage\n",
            "models becoming a basic tool in\n",
            "manyNLPtasks[Devlinetal.,2018,\n",
            "Radfordetal.,2019,Liuetal.,2019].\n",
            "While these models lead to signifi-\n",
            "cant improvement, they often have\n",
            "severalhundredmillionparameters\n",
            "andcurrentresearch1onpre-trained\n",
            "models indicates that training even\n",
            "largermodelsstillleadstobetterper-\n",
            "formancesondownstreamtasks.\n",
            "Figure 1: Parameter counts of several recently released\n",
            "The trend toward bigger models\n",
            "pretrainedlanguagemodels.\n",
            "raisesseveralconcerns. Firstisthe\n",
            "environmentalcostofexponentiallyscalingthesemodels’computationalrequirementsasmentioned\n",
            "inSchwartzetal.[2019],Strubelletal.[2019]. Second,whileoperatingthesemodelson-device\n",
            "inreal-timehasthepotentialtoenablenovelandinterestinglanguageprocessingapplications,the\n",
            "growingcomputationalandmemoryrequirementsofthesemodelsmayhamperwideadoption.\n",
            "1SeeforinstancetherecentlyreleasedMegatronLM(https://nv-adlr.github.io/MegatronLM)\n",
            "EMC^2:5thEditionCo-locatedwithNeurIPS’19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "tokens = nltk.sent_tokenize(txt)\n",
        "for i in tokens:\n",
        "  print(i,\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7ROP8lNKMYs",
        "outputId": "f8d31c36-3330-4162-994e-27fbdbe1b3e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT, a distilled version of BERT: smaller,\n",
            "faster, cheaper and lighter\n",
            "VictorSANH,LysandreDEBUT,JulienCHAUMOND,ThomasWOLF\n",
            "HuggingFace\n",
            "{victor,lysandre,julien,thomas}@huggingface.co\n",
            "0202\n",
            "Abstract\n",
            "raM\n",
            "AsTransferLearningfromlarge-scalepre-trainedmodelsbecomesmoreprevalent\n",
            "inNaturalLanguageProcessing(NLP),operatingtheselargemodelsinon-the-\n",
            "edgeand/orunderconstrainedcomputationaltrainingorinferencebudgetsremains\n",
            "challenging. \n",
            "\n",
            "In this work, we propose a method to pre-train a smaller general- 1\n",
            "purposelanguagerepresentationmodel,calledDistilBERT,whichcanthenbefine-\n",
            "]LC.sc[\n",
            "tunedwithgoodperformancesonawiderangeoftaskslikeitslargercounterparts. \n",
            "\n",
            "Whilemostpriorworkinvestigatedtheuseofdistillationforbuildingtask-specific\n",
            "models,weleverageknowledgedistillationduringthepre-trainingphaseandshow\n",
            "thatitispossibletoreducethesizeofaBERTmodelby40%,whileretaining97%\n",
            "ofitslanguageunderstandingcapabilitiesandbeing60%faster. \n",
            "\n",
            "Toleveragethe\n",
            "inductivebiaseslearnedbylargermodelsduringpre-training,weintroduceatriple\n",
            "losscombininglanguagemodeling,distillationandcosine-distancelosses. \n",
            "\n",
            "Our 4v80110.0191:viXra\n",
            "smaller, fasterandlightermodelischeapertopre-trainandwedemonstrateits\n",
            "capabilitiesforon-devicecomputationsinaproof-of-conceptexperimentanda\n",
            "comparativeon-devicestudy. \n",
            "\n",
            "1 Introduction\n",
            "Thelasttwoyearshaveseentherise\n",
            "ofTransferLearningapproachesin\n",
            "NaturalLanguageProcessing(NLP)\n",
            "withlarge-scalepre-trainedlanguage\n",
            "models becoming a basic tool in\n",
            "manyNLPtasks[Devlinetal.,2018,\n",
            "Radfordetal.,2019,Liuetal.,2019]. \n",
            "\n",
            "While these models lead to signifi-\n",
            "cant improvement, they often have\n",
            "severalhundredmillionparameters\n",
            "andcurrentresearch1onpre-trained\n",
            "models indicates that training even\n",
            "largermodelsstillleadstobetterper-\n",
            "formancesondownstreamtasks. \n",
            "\n",
            "Figure 1: Parameter counts of several recently released\n",
            "The trend toward bigger models\n",
            "pretrainedlanguagemodels. \n",
            "\n",
            "raisesseveralconcerns. \n",
            "\n",
            "Firstisthe\n",
            "environmentalcostofexponentiallyscalingthesemodels’computationalrequirementsasmentioned\n",
            "inSchwartzetal.[2019],Strubelletal.[2019]. \n",
            "\n",
            "Second,whileoperatingthesemodelson-device\n",
            "inreal-timehasthepotentialtoenablenovelandinterestinglanguageprocessingapplications,the\n",
            "growingcomputationalandmemoryrequirementsofthesemodelsmayhamperwideadoption. \n",
            "\n",
            "1SeeforinstancetherecentlyreleasedMegatronLM(https://nv-adlr.github.io/MegatronLM)\n",
            "EMC^2:5thEditionCo-locatedwithNeurIPS’19 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "def sentence_to_clean(sentence,stopwords=False):\n",
        "  sentence = sentence.lower().strip()\n",
        "  sentence = re.sub(r'[^a-z0-9\\s]','',sentence)\n",
        "  if stopwords:\n",
        "    sentence = remove_stopwords(sentence)\n",
        "  return sentence\n",
        "\n",
        "def sentences_to_clean(tokens, stopwords=False):\n",
        "  cleaned_sentences = []\n",
        "  for row in tokens:\n",
        "    cleaned_sentences.append(sentence_to_clean(row,stopwords))\n",
        "  return cleaned_sentences"
      ],
      "metadata": {
        "id": "dTEib-J7KgHo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = sentences_to_clean(tokens, stopwords=True)\n",
        "with_stopwords = sentences_to_clean(tokens,stopwords=False)\n",
        "print(cleaned_sentences)\n",
        "print(with_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NlqfkObMQ2W",
        "outputId": "1c685883-5307-4663-c419-122aa604a265"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['distilbert distilled version bert smaller faster cheaper lighter victorsanhlysandredebutjulienchaumondthomaswolf huggingface victorlysandrejulienthomashuggingfaceco 0202 abstract ram astransferlearningfromlargescalepretrainedmodelsbecomesmoreprevalent innaturallanguageprocessingnlpoperatingtheselargemodelsinonthe edgeandorunderconstrainedcomputationaltrainingorinferencebudgetsremains challenging', 'work propose method pretrain smaller general 1 purposelanguagerepresentationmodelcalleddistilbertwhichcanthenbefine lcsc tunedwithgoodperformancesonawiderangeoftaskslikeitslargercounterparts', 'whilemostpriorworkinvestigatedtheuseofdistillationforbuildingtaskspecific modelsweleverageknowledgedistillationduringthepretrainingphaseandshow thatitispossibletoreducethesizeofabertmodelby40whileretaining97 ofitslanguageunderstandingcapabilitiesandbeing60faster', 'toleveragethe inductivebiaseslearnedbylargermodelsduringpretrainingweintroduceatriple losscombininglanguagemodelingdistillationandcosinedistancelosses', '4v801100191vixra smaller fasterandlightermodelischeapertopretrainandwedemonstrateits capabilitiesforondevicecomputationsinaproofofconceptexperimentanda comparativeondevicestudy', '1 introduction thelasttwoyearshaveseentherise oftransferlearningapproachesin naturallanguageprocessingnlp withlargescalepretrainedlanguage models basic tool manynlptasksdevlinetal2018 radfordetal2019liuetal2019', 'models lead signifi improvement severalhundredmillionparameters andcurrentresearch1onpretrained models indicates training largermodelsstillleadstobetterper formancesondownstreamtasks', 'figure 1 parameter counts recently released trend bigger models pretrainedlanguagemodels', 'raisesseveralconcerns', 'firstisthe environmentalcostofexponentiallyscalingthesemodelscomputationalrequirementsasmentioned inschwartzetal2019strubelletal2019', 'secondwhileoperatingthesemodelsondevice inrealtimehasthepotentialtoenablenovelandinterestinglanguageprocessingapplicationsthe growingcomputationalandmemoryrequirementsofthesemodelsmayhamperwideadoption', '1seeforinstancetherecentlyreleasedmegatronlmhttpsnvadlrgithubiomegatronlm emc25theditioncolocatedwithneurips19']\n",
            "['distilbert a distilled version of bert smaller\\nfaster cheaper and lighter\\nvictorsanhlysandredebutjulienchaumondthomaswolf\\nhuggingface\\nvictorlysandrejulienthomashuggingfaceco\\n0202\\nabstract\\nram\\nastransferlearningfromlargescalepretrainedmodelsbecomesmoreprevalent\\ninnaturallanguageprocessingnlpoperatingtheselargemodelsinonthe\\nedgeandorunderconstrainedcomputationaltrainingorinferencebudgetsremains\\nchallenging', 'in this work we propose a method to pretrain a smaller general 1\\npurposelanguagerepresentationmodelcalleddistilbertwhichcanthenbefine\\nlcsc\\ntunedwithgoodperformancesonawiderangeoftaskslikeitslargercounterparts', 'whilemostpriorworkinvestigatedtheuseofdistillationforbuildingtaskspecific\\nmodelsweleverageknowledgedistillationduringthepretrainingphaseandshow\\nthatitispossibletoreducethesizeofabertmodelby40whileretaining97\\nofitslanguageunderstandingcapabilitiesandbeing60faster', 'toleveragethe\\ninductivebiaseslearnedbylargermodelsduringpretrainingweintroduceatriple\\nlosscombininglanguagemodelingdistillationandcosinedistancelosses', 'our 4v801100191vixra\\nsmaller fasterandlightermodelischeapertopretrainandwedemonstrateits\\ncapabilitiesforondevicecomputationsinaproofofconceptexperimentanda\\ncomparativeondevicestudy', '1 introduction\\nthelasttwoyearshaveseentherise\\noftransferlearningapproachesin\\nnaturallanguageprocessingnlp\\nwithlargescalepretrainedlanguage\\nmodels becoming a basic tool in\\nmanynlptasksdevlinetal2018\\nradfordetal2019liuetal2019', 'while these models lead to signifi\\ncant improvement they often have\\nseveralhundredmillionparameters\\nandcurrentresearch1onpretrained\\nmodels indicates that training even\\nlargermodelsstillleadstobetterper\\nformancesondownstreamtasks', 'figure 1 parameter counts of several recently released\\nthe trend toward bigger models\\npretrainedlanguagemodels', 'raisesseveralconcerns', 'firstisthe\\nenvironmentalcostofexponentiallyscalingthesemodelscomputationalrequirementsasmentioned\\ninschwartzetal2019strubelletal2019', 'secondwhileoperatingthesemodelsondevice\\ninrealtimehasthepotentialtoenablenovelandinterestinglanguageprocessingapplicationsthe\\ngrowingcomputationalandmemoryrequirementsofthesemodelsmayhamperwideadoption', '1seeforinstancetherecentlyreleasedmegatronlmhttpsnvadlrgithubiomegatronlm\\nemc25theditioncolocatedwithneurips19']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "sent_words = [[word for word in document.split()]\n",
        "              for document in with_stopwords]\n",
        "from gensim import corpora\n",
        "dictio = corpora.Dictionary(sent_words)\n",
        "for k,v in dictio.items():\n",
        "  print(k, ':' , v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V5RvQL4NKES",
        "outputId": "46970734-215a-44e8-ec6a-69b453b96d87"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : 0202\n",
            "1 : a\n",
            "2 : abstract\n",
            "3 : and\n",
            "4 : astransferlearningfromlargescalepretrainedmodelsbecomesmoreprevalent\n",
            "5 : bert\n",
            "6 : challenging\n",
            "7 : cheaper\n",
            "8 : distilbert\n",
            "9 : distilled\n",
            "10 : edgeandorunderconstrainedcomputationaltrainingorinferencebudgetsremains\n",
            "11 : faster\n",
            "12 : huggingface\n",
            "13 : innaturallanguageprocessingnlpoperatingtheselargemodelsinonthe\n",
            "14 : lighter\n",
            "15 : of\n",
            "16 : ram\n",
            "17 : smaller\n",
            "18 : version\n",
            "19 : victorlysandrejulienthomashuggingfaceco\n",
            "20 : victorsanhlysandredebutjulienchaumondthomaswolf\n",
            "21 : 1\n",
            "22 : general\n",
            "23 : in\n",
            "24 : lcsc\n",
            "25 : method\n",
            "26 : pretrain\n",
            "27 : propose\n",
            "28 : purposelanguagerepresentationmodelcalleddistilbertwhichcanthenbefine\n",
            "29 : this\n",
            "30 : to\n",
            "31 : tunedwithgoodperformancesonawiderangeoftaskslikeitslargercounterparts\n",
            "32 : we\n",
            "33 : work\n",
            "34 : modelsweleverageknowledgedistillationduringthepretrainingphaseandshow\n",
            "35 : ofitslanguageunderstandingcapabilitiesandbeing60faster\n",
            "36 : thatitispossibletoreducethesizeofabertmodelby40whileretaining97\n",
            "37 : whilemostpriorworkinvestigatedtheuseofdistillationforbuildingtaskspecific\n",
            "38 : inductivebiaseslearnedbylargermodelsduringpretrainingweintroduceatriple\n",
            "39 : losscombininglanguagemodelingdistillationandcosinedistancelosses\n",
            "40 : toleveragethe\n",
            "41 : 4v801100191vixra\n",
            "42 : capabilitiesforondevicecomputationsinaproofofconceptexperimentanda\n",
            "43 : comparativeondevicestudy\n",
            "44 : fasterandlightermodelischeapertopretrainandwedemonstrateits\n",
            "45 : our\n",
            "46 : basic\n",
            "47 : becoming\n",
            "48 : introduction\n",
            "49 : manynlptasksdevlinetal2018\n",
            "50 : models\n",
            "51 : naturallanguageprocessingnlp\n",
            "52 : oftransferlearningapproachesin\n",
            "53 : radfordetal2019liuetal2019\n",
            "54 : thelasttwoyearshaveseentherise\n",
            "55 : tool\n",
            "56 : withlargescalepretrainedlanguage\n",
            "57 : andcurrentresearch1onpretrained\n",
            "58 : cant\n",
            "59 : even\n",
            "60 : formancesondownstreamtasks\n",
            "61 : have\n",
            "62 : improvement\n",
            "63 : indicates\n",
            "64 : largermodelsstillleadstobetterper\n",
            "65 : lead\n",
            "66 : often\n",
            "67 : severalhundredmillionparameters\n",
            "68 : signifi\n",
            "69 : that\n",
            "70 : these\n",
            "71 : they\n",
            "72 : training\n",
            "73 : while\n",
            "74 : bigger\n",
            "75 : counts\n",
            "76 : figure\n",
            "77 : parameter\n",
            "78 : pretrainedlanguagemodels\n",
            "79 : recently\n",
            "80 : released\n",
            "81 : several\n",
            "82 : the\n",
            "83 : toward\n",
            "84 : trend\n",
            "85 : raisesseveralconcerns\n",
            "86 : environmentalcostofexponentiallyscalingthesemodelscomputationalrequirementsasmentioned\n",
            "87 : firstisthe\n",
            "88 : inschwartzetal2019strubelletal2019\n",
            "89 : growingcomputationalandmemoryrequirementsofthesemodelsmayhamperwideadoption\n",
            "90 : inrealtimehasthepotentialtoenablenovelandinterestinglanguageprocessingapplicationsthe\n",
            "91 : secondwhileoperatingthesemodelsondevice\n",
            "92 : 1seeforinstancetherecentlyreleasedmegatronlmhttpsnvadlrgithubiomegatronlm\n",
            "93 : emc25theditioncolocatedwithneurips19\n"
          ]
        }
      ]
    }
  ]
}